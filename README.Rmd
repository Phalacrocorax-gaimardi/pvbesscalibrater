---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# pvbesscalibrater

<!-- badges: start -->
<!-- badges: end -->

*pvbesscalibrater* creates a micro-calibrated agent-based-model for household adoption of solar PV and battery energy storage systems (BESS). The micro-calibrated model consists of a set of empirical partial utilities and associated weights for each agent. These outputs are passed to *pvbessmicrosimr* that can run alternative cost and policy scenarios. 

Tasks handled by *pvbesscalibrater* include:

* a mapping stated likelihood-to-adopt (LTA) Likert scores to utilities 
* selection of static and dynamic features
* learning of partial utilities and associated weights using gradient boosting (*xgboost*)
* derivation of regularised model weights


## Installation

Install the latest development version of *pvbesscalibrater*:

```{r message=FALSE}
remotes::install_github("Phalacrocorax-gaimardi/pvbesscalibrater")
```
Load packages
```{r message=FALSE}
library(pvbesscalibrater)
library(tidyverse)
```

## Data

*pvbessmicrosimr* includes two survey datasets *pv_survey* and *pv_survey_oo* where the latter dataset is restricted to owner-occupiers only.
```{r}
dim(pv_survey_oo)
```
The meaning of the features present in *pv_survey* and *pv_survey_oo* are described by *pv_questiobs* and *pv_qanda*.
```{r}
pv_questions
```

Some features retained in *pv_survey_oo* are unlikely to be relevant for model-building. *pvbessmicrosimr* has initial feature selection built-in. By default, dynamic features are __q14__ (highest 2023 bill) as financial dynamic variable and __q45__ as dynamic social influence variable. All other retained features are treated as static.
```{r message=FALSE}
pv_data <- feature_select(pv_survey_oo,recode_bills=F,drop_lowestbills = T)
dim(pv_data)
```
LTA Likert scores __q46_5__ can be replaced by utilities for adoption (__u__) using simple linear scaling. __u__ becomes the dependent variable in a regression model. *pvbesscalibrater* allows for potential non-linearity in this mapping via a hypothetical bias parameter $\epsilon$. $\epsilon=1$ means no hypothetical bias. $\epsilon=0.7$ is a reasonable initial choice. Macro-calibration using *pvbessmicrosimr* further corrects for hypothetical bias.

```{r}
pv_data <- transform_to_utils(pv_data,epsilon=0.7)
```
### Build
Model-building means regressing the utilities **u** onto the other features present in *pv_data* using *xgboost*. A boosted tree object *bst* is obtained using *pvbesscalibrater::get_boosted_tree_model* with 5-fold cross validation to optimise model complexity:
```{r results="hide", message=FALSE,warnings=FALSE}
bst <- get_boosted_tree_model(pv_data)
```
SHAP scores are extracted for each agent and feature from *bst*.
```{r message=FALSE}
shap_scores_long <- get_shap_scores(pv_data,bst)

```
For dynamic features, the SHAP scores are interpreted directly as mean empirical partial utilities for each agent $i$. SHAP scores of static features are aggregated into a barrier or threshold term $\theta_i$.

The empirical utilities are
```{r results="hide", message=FALSE}
empirical_utils <- get_empirical_partial_utilities(shap_scores_long, regularisation = 1)

```

The model weights are
```{r message=FALSE}
regularised_weights <- get_model_weights(shap_scores_long, regularisation = 1)

```

In the above examples, model weights are *regularised* so that weights for dynamic partial utilities are always positive. The sign of the barrier weights can have either sign. $regularisation < 1$ can result in some negative weights. No regularisation ($regularisation = 0$) leads to a wider spread and 10-15% negative weights which is undesirable.

### Visualisation

Examples of empirical partial utilities for electricity bill category (**q14**) and number of associates that have installed solar PV (**q45**).

```{r echo = FALSE,message=FALSE}
empirical_utils %>% filter(question_code != "theta", response_code != 13) %>% ggplot(aes(response_code,du_average))+geom_point() + facet_wrap(.~question_code, scales="free_x")
```


```{r  echo = FALSE, message=FALSE}
library(GGally)
ggpairs(get_model_weights(shap_scores_long,"mean", 1)[,2:4] )
```
